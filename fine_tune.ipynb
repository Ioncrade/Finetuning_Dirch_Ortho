{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c26e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f5124",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n",
    "    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n",
    "    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n",
    "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754e5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76402eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "dataset = load_from_disk(\"path\")   # folder that contains data-00000-of-00001.arrow etc.\n",
    "\n",
    "\n",
    "print(f\"Loaded {len(dataset)} examples\")\n",
    "print(\"Columns:\", dataset.column_names)      # should be only: input_ids, attention_mask (and maybe labels if you added them)\n",
    "print(\"Features:\", dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3d085d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample decoded:\")\n",
    "print(tokenizer.decode(dataset[0][\"input_ids\"])[:800] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636961d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvidentialTrainer(SFTTrainer):\n",
    "   \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None): \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        \n",
    "        # Double check logic to prevent silent failures\n",
    "        if logits is None:\n",
    "            raise ValueError(\"Logits are None! Ensure os.environ['UNSLOTH_RETURN_LOGITS']='1' is set at the top of the script.\")\n",
    "\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # Shift logits and labels for Next-Token Prediction\n",
    "        shift_logits = logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "        # --- DIRICHLET LOGIC START ---\n",
    "        # 1. Convert Logits to Evidence (must be non-negative)\n",
    "        evidence = F.softplus(shift_logits)\n",
    "        \n",
    "        # 2. Calculate Dirichlet Parameters (Alpha)\n",
    "        alpha = evidence + 1.0\n",
    "        \n",
    "        # 3. Calculate Dirichlet Strength (Total Evidence)\n",
    "        S = torch.sum(alpha, dim=-1, keepdim=True)\n",
    "        \n",
    "        # 4. Calculate Expected Probability: E[p] = alpha / S\n",
    "        expected_probs = alpha / S\n",
    "        \n",
    "        # 5. Loss: Negative Log Likelihood of the Expected Probability\n",
    "        # We use log(expected_probs) to be compatible with nll_loss\n",
    "        log_probs = torch.log(expected_probs + 1e-8) \n",
    "\n",
    "        # Flatten for NLL Loss\n",
    "        loss = F.nll_loss(\n",
    "            log_probs.view(-1, log_probs.size(-1)), \n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        # --- DIRICHLET LOGIC END ---\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = EvidentialTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,                  # already tokenized â†’ super fast\n",
    "    dataset_text_field=None,                # IMPORTANT: None because we have input_ids already\n",
    "    max_seq_length=max_seq_length,          # can even be None now\n",
    "    packing=False,                          # must be False (we already did the packing/splitting)\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,     \n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        max_steps=60,                       \n",
    "        learning_rate=2e-4,\n",
    "        fp16=False,\n",
    "        bf16=True,                          \n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",                \n",
    "        weight_decay=0.001,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=4,          \n",
    "        remove_unused_columns=True,\n",
    "        dataloader_pin_memory=True,\n",
    "        \n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"Starting training from pre-processed arrow file...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d95527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Save the adapters first just in case\n",
    "trainer.save_model(\"temp_adapters\")\n",
    "from peft import PeftModel\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# 3. Save the full merged model\n",
    "save_path = \"evidential_model_merged\"\n",
    "merged_model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Full merged model saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886957c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import OFTConfig, TaskType, get_peft_model\n",
    "\n",
    "oft_config = OFTConfig(\n",
    "    r=8,                           # Rank (similar to LoRA)\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    module_dropout=0.05,           # Helps prevent overfitting\n",
    "    coft=True,                     # \"Constrained\" OFT - strictly enforces orthogonality\n",
    "    eps=6e-5,                      \n",
    ")\n",
    "\n",
    "model = get_peft_model(model, oft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch\n",
    "\n",
    "class EvidentialOrthogonalTrainer(EvidentialTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # 1. Run the standard forward pass\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # 2. Get the standard loss (NLL or Evidential Loss)\n",
    "        loss = outputs.loss if isinstance(outputs, dict) else outputs[0]\n",
    "      \n",
    "        hidden_states = outputs.hidden_states[-1] # (Batch, Seq, Dim)\n",
    "        \n",
    "        # Normalize vectors\n",
    "        norm_hidden = torch.nn.functional.normalize(hidden_states, p=2, dim=-1)\n",
    "        \n",
    "        # Calculate cosine similarity matrix\n",
    "        # (Batch, Seq, Dim) x (Batch, Dim, Seq) -> (Batch, Seq, Seq)\n",
    "        similarity = torch.bmm(norm_hidden, norm_hidden.transpose(1, 2))\n",
    "        \n",
    "        # We want the off-diagonal elements (token-to-token similarity) to be low (orthogonal)\n",
    "        # to prevent \"mode collapse\" or repetitive loops (hallucination)\n",
    "        identity = torch.eye(similarity.size(1)).to(similarity.device)\n",
    "        ortho_loss = torch.norm(similarity - identity, p='fro')\n",
    "        \n",
    "        # Add to total loss with a small coefficient (lambda)\n",
    "        total_loss = loss + (0.1 * ortho_loss)\n",
    "        \n",
    "        return (total_loss, outputs) if return_outputs else total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from trl import SFTConfig # or use TrainingArguments directly\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"models\", # New output folder for the 2nd round\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=60,                     # Train for 60 MORE steps\n",
    "    learning_rate=5e-5,               # Lower LR is better for 2nd round (was 2e-4)\n",
    "    fp16=False,\n",
    "    bf16=True,\n",
    "    logging_steps=1,\n",
    "    optim=\"adamw_8bit\",\n",
    "    save_strategy=\"steps\",            # Save checkpoints this time\n",
    "    save_steps=20,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Re-initialize your custom Trainer\n",
    "trainer = EvidentialTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,            # You can use the same dataset or a new one\n",
    "    dataset_text_field=None,\n",
    "    max_seq_length=max_seq_length,\n",
    "    packing=False,\n",
    "    args=args\n",
    ")\n",
    "\n",
    "print(\"Resuming training on the saved Orthogonal model...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ca419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path = \"evidential_model_orthogonal\"\n",
    "\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "trainer.model.config.save_pretrained(save_path)\n",
    "\n",
    "print(f\"Orthogonal adapters saved to: {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
